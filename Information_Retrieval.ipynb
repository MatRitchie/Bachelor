{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information_Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Uc0SiCTZut5G",
        "FBPkmzcEhZgL",
        "IxCTtisSYf8p"
      ],
      "authorship_tag": "ABX9TyM1ygfx9RVfVPkEE1o7Z4Cp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatRitchie/Bachelor/blob/main/Information_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6KAVjqEvWtj"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdnPi9gHaf5S",
        "outputId": "ea936625-aa7c-4283-d32f-6a279e11910b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/95/d1d606fff85b537ba6dd133ed998ab62bf0c950feb6df2d101c0ec804ca6/allennlp-1.1.0-py3-none-any.whl (485kB)\n",
            "\r\u001b[K     |▊                               | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 6.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 6.1MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 6.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 133kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 143kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 153kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 163kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 174kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 184kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 194kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 204kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 215kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 225kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 235kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 245kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 256kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 266kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 276kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 286kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 296kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 307kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 317kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 327kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 337kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 348kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 358kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 368kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 378kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 389kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 399kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 409kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 419kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 430kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 440kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 450kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 460kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 471kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 481kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 491kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: spacy<2.4,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 18.4MB/s \n",
            "\u001b[?25hCollecting overrides==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Collecting boto3<2.0,>=1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/34/bdef270a8ae5cbf2b89a2dfe9a477cd63ae5ea9a5740e6412c6c320cc2da/boto3-1.15.15-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.7)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/70/ed1ba808a87d896b9f4d25400dda54e089ca7a97e87cee620b3744997c89/jsonnet-0.16.0.tar.gz (256kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 21.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<1.7.0,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.6.0+cu101)\n",
            "Collecting transformers<3.1,>=3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 29.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->allennlp) (1.15.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (50.3.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (2.0.0)\n",
            "Collecting botocore<1.19.0,>=1.18.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/83/db265be17aa6e1a8b24cd9032752a5c46d504a9ed654121793c98bbfbb9a/botocore-1.18.15-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 68.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<1.7.0,>=1.6.0->allennlp) (0.16.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 76.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.1,>=3.0->allennlp) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 70.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.1,>=3.0->allennlp) (20.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.16.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (20.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.19.0,>=1.18.15->boto3<2.0,>=1.14->allennlp) (2.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.1,>=3.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.1,>=3.0->allennlp) (2.4.7)\n",
            "Building wheels for collected packages: overrides, jsonnet, sacremoses\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=1b3d6808a367256fe76c1c4e46b838a2b4f4a11933c8e43b3bca52631e1ae0d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.16.0-cp36-cp36m-linux_x86_64.whl size=3321570 sha256=364b296c6852c1918e040ca568e925c5719332db4b9fc016f963c0acc2185c3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a9/43/bc5e0463deeec89dfca928a2a64595f1bdb520c891f6fbd09c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f59afd72737b4a9fcb15d977481ce5c89521cafdabe3fe82355296cc65f44fb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built overrides jsonnet sacremoses\n",
            "Installing collected packages: tensorboardX, overrides, jsonpickle, jmespath, botocore, s3transfer, boto3, jsonnet, tokenizers, sacremoses, sentencepiece, transformers, allennlp\n",
            "Successfully installed allennlp-1.1.0 boto3-1.15.15 botocore-1.18.15 jmespath-0.10.0 jsonnet-0.16.0 jsonpickle-1.4.1 overrides-3.1.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.91 tensorboardX-2.1 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTrn-NBAQKUc",
        "outputId": "e7bfbd4a-2b2b-4e93-8e10-8e41512a0d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!pip install registrable"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting registrable\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/47/6370d9389664f1af31de4048934faeb681035a079ddc3d284a366875e755/registrable-0.0.4-py3-none-any.whl\n",
            "Installing collected packages: registrable\n",
            "Successfully installed registrable-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFwuz9QJd3qh",
        "cellView": "form"
      },
      "source": [
        "#@title Import resources\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from typing import Dict, Iterable, List, Tuple\n",
        "import logging \n",
        "from overrides import overrides\n",
        "\n",
        "from allennlp.common.file_utils import cached_path\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import allennlp\n",
        "from allennlp.data import DataLoader, DatasetReader, Instance, Vocabulary\n",
        "from allennlp.data.fields import LabelField, TextField, ArrayField, ListField\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
        "from allennlp.nn import util,Activation\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.training.optimizers import AdamOptimizer\n",
        "from allennlp.training.trainer import Trainer, GradientDescentTrainer\n",
        "from allennlp.training.util import evaluate\n",
        "from google.colab import files\n",
        "from allennlp.modules.seq2vec_encoders.cls_pooler import ClsPooler\n",
        "from allennlp.common import FromParams #For config files\n",
        "from allennlp.common.checks import ConfigurationError\n",
        "from registrable import Registrable\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "\n",
        "from allennlp.training.metrics.metric import Metric\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import IntTensor\n",
        "from allennlp.data import allennlp_collate\n",
        "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder, TimeDistributed\n",
        "from allennlp.nn import InitializerApplicator, util\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "\n",
        "from allennlp.training.metrics import CategoricalAccuracy, BooleanAccuracy, Auc, F1Measure, FBetaMeasure, PearsonCorrelation\n",
        "\n",
        "from torch import Tensor\n",
        "from allennlp.common.registrable import Registrable\n",
        "from allennlp.data import TextFieldTensors\n",
        "\n",
        "from allennlp.modules.token_embedders import (\n",
        "    Embedding,\n",
        "    TokenCharactersEncoder,\n",
        "    ElmoTokenEmbedder,\n",
        "    PretrainedTransformerEmbedder,\n",
        "    PretrainedTransformerMismatchedEmbedder,\n",
        ")\n",
        "\n",
        "from allennlp.data.token_indexers import (\n",
        "    TokenIndexer,\n",
        "    SingleIdTokenIndexer,\n",
        "    TokenCharactersIndexer,\n",
        "    ELMoTokenCharactersIndexer,\n",
        "    PretrainedTransformerIndexer,\n",
        "    PretrainedTransformerMismatchedIndexer,\n",
        ")\n",
        "\n",
        "from allennlp.data.tokenizers import (\n",
        "    Token, \n",
        "    Tokenizer,\n",
        "    CharacterTokenizer,\n",
        "    PretrainedTransformerTokenizer,\n",
        "    SpacyTokenizer,\n",
        "    WhitespaceTokenizer,\n",
        ")\n",
        "\n",
        "from allennlp.modules.attention import (\n",
        "    DotProductAttention,\n",
        "    BilinearAttention,\n",
        "    LinearAttention,\n",
        ")\n",
        "from allennlp.modules.matrix_attention import (\n",
        "    DotProductMatrixAttention,\n",
        "    BilinearMatrixAttention,\n",
        "    LinearMatrixAttention,\n",
        ")\n",
        "from allennlp.nn import Activation\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDJU0XZOSFuw",
        "cellView": "form",
        "outputId": "02d3e25c-5655-4b95-bbe4-406b8b3dcc17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Mounting google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHc25opahROX",
        "cellView": "form"
      },
      "source": [
        "#@title Downloading kaggle database\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "# /content/gdrive/My Drive/Kaggle is the path where kaggle.json is present in the Google Drive\n",
        "\n",
        "#changing the working directory\n",
        "%cd /content/gdrive/My Drive/Kaggle\n",
        "!kaggle datasets download -d pratiksaha198/lyrics-generation\n",
        "\n",
        "#unzipping the zip files and deleting the zip files\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY0uiAZkTXoW",
        "cellView": "form"
      },
      "source": [
        "#@title Check GPU, devices, memory\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "#Listing devices used\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "\n",
        "#Memory resources available\n",
        "!cat /proc/meminfo\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc0SiCTZut5G"
      },
      "source": [
        "#Load data\n",
        "\n",
        "Currently -> loading a dataset og artist, song title, lyrics from a kaggle datset, which is stored in my google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgI8Vs_Peyq_",
        "outputId": "729c8949-baa7-436d-a224-379fd213369d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "#Loading of Kaggle dataset\n",
        "\n",
        "path = '/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_train.csv'\n",
        "data = pd.read_csv(path)\n",
        "data.head()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Artist</th>\n",
              "      <th>Title</th>\n",
              "      <th>Lyric</th>\n",
              "      <th>Procentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adam Melchor</td>\n",
              "      <td>calm  forward</td>\n",
              "      <td>bugs will always bite me needles will never fr...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hozier</td>\n",
              "      <td>nina cried power</td>\n",
              "      <td>its not the waking its the rising it is the g...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Issac Gracie</td>\n",
              "      <td>​reverie</td>\n",
              "      <td>hey babe what do you expect me to say i could...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ed Sheeran</td>\n",
              "      <td>little lady</td>\n",
              "      <td>listen little lady this is just the worst way...</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>John Mayer</td>\n",
              "      <td>emoji of a wave</td>\n",
              "      <td>oh honey you dont have to try so hard to hurt...</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Artist  ... Procentage\n",
              "0  Adam Melchor  ...      100.0\n",
              "1        Hozier  ...      100.0\n",
              "2  Issac Gracie  ...        0.0\n",
              "3    Ed Sheeran  ...      100.0\n",
              "4    John Mayer  ...       50.0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP45hKHArJlm"
      },
      "source": [
        "# Making a configuration file \n",
        "This takes to construction parameters from different objects, and then put them in a dictionary in a JSON file\n",
        "\n",
        "Motivation to keep configuration of an object seperate from the implementation code. \n",
        "Dependency injection: all of the instance variables are given a contructor parameters. \n",
        "\n",
        "FromParams-> constructing arguments for an object from JSON\n",
        "It is neccesary to use type annotations for constructor arguments, such that it can be parsed correctly from JSON. The object is then created from a JSON dictionary instead of the constructor \n",
        "\n",
        "In AllenNLP all classes inherit from FromParam, such that one configuration file recursivly can be used to specify an entire experiment. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBPkmzcEhZgL"
      },
      "source": [
        "# Defining input and output\n",
        "There is an instance object, which consists of one or more fields. A field represents one piece of data, which can be either input data or output data. A field gets converted into a tensor, and fed to the model.\n",
        "\n",
        "In the case of information retrivalm the input will be a query consisting of a song title, and the output will be the song lyric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj5WaoFBiYil",
        "cellView": "code"
      },
      "source": [
        "#@title Default title text\n",
        "\n",
        "#The input is song lyrics \n",
        "song_lyric: TextField\n",
        "\n",
        "#This input will contain all of the song title eg. [title1, title2, title3...]\n",
        "song_title: ListField \n",
        "\n",
        "#This is a one hot vector, which tells what title is correct for the lyric eg. [0, 0, 1, 0, 0]\n",
        "label: ArrayField "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9C7Ve1li6Wn"
      },
      "source": [
        "# Reading data\n",
        "We want to read the data, and represent it with an internal structure. For that purpose we want to build a DatasetReader. We want to transform rw data into instance which match the input/output specifacations. \n",
        "\n",
        "Input: lyric, list of all titles, one hot vector label\n",
        "\n",
        "output: ranking \n",
        "\n",
        "An instance should consist of 1 song and all titles and a label, which is a one hot vector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_dH0ciYxB1Y"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "#@DatasetReader.register(\"ranker\")\n",
        "class DatasetReaderRanker(DatasetReader):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer = None,\n",
        "        token_indexers: Dict[str, TokenIndexer] = None,\n",
        "        max_tokens: int = None,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "        self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
        "        self.max_tokens = max_tokens\n",
        "        \n",
        "\n",
        "    @overrides\n",
        "    def _read(self, file_path: str):\n",
        "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
        "        feature_names = ['artist', 'title','lyric', 'percentage']\n",
        "        df = pd.read_csv(cached_path(file_path),header=None, skiprows=1, names = feature_names)            #using pandas to open file \n",
        "        df = df.dropna(thresh=df.shape[1])\n",
        "\n",
        "\n",
        "        title_list = df['title'].tolist() #getting a list of all the titles ['title1', 'title2'...]\n",
        "        df.drop(columns=['artist', 'title', 'percentage'], axis=1, inplace=True)      #columns, which won't be used \n",
        "\n",
        "        res = []\n",
        "        for i in range(df.shape[0]): \n",
        "          zero_list = [0] * df.shape[0]\n",
        "          zero_list[i] = 1\n",
        "          res.append(zero_list)\n",
        "       \n",
        "        i = 0\n",
        "        for row in df.to_dict(orient='records'):       #‘records’ : list like [{column -> value}, … , {column -> value}]\n",
        "            yield self.text_to_instance( res[i], title_list,**row)\n",
        "            i = i + 1\n",
        "            \n",
        "    def _make_textfield(self, text: str):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        if self.max_tokens:\n",
        "            tokens = tokens[:self.max_tokens]\n",
        "        return TextField(tokens, token_indexers=self.token_indexers)\n",
        "\n",
        "    @overrides\n",
        "    def text_to_instance(\n",
        "        self,\n",
        "        labels: List[int],\n",
        "        title_options: List[str],\n",
        "        lyric: str, \n",
        "    ) -> Instance: \n",
        "        \n",
        "        if labels:\n",
        "            assert all(l >= 0 for l in labels)\n",
        "            assert all((l == 0) for l in labels[len(title_options):])\n",
        "            labels = labels[:len(title_options)]\n",
        "        \n",
        "        \n",
        "        lyric_field = self._make_textfield(lyric)\n",
        "        options_field = ListField([self._make_textfield(o) for o in title_options])\n",
        "      \n",
        "        fields = { 'lyric': lyric_field, 'title_options': options_field }\n",
        "\n",
        "        if labels:\n",
        "            labels = list(map(float, filter(lambda x: not pd.isnull(x), labels)))            \n",
        "            fields['labels'] = ArrayField(np.array(labels), padding_value=-1)\n",
        "        \n",
        "        return Instance(fields)\n",
        "\n",
        "  "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8vbXT-ij_2J"
      },
      "source": [
        "# Making a model \n",
        "The model will take a bunch of instances, then it will predict the output from the input, and then it will compute loss\n",
        "\n",
        "Taken parameters, which you want to train/chang in as constructor parameters makes alot of sense, since they will be easier to change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rpC_s6auCdJ"
      },
      "source": [
        "def first_nonzero(t):\n",
        "    t = t.masked_fill(t != 0, 1)\n",
        "    idx = torch.arange(t.size(-1), 0, -1).type_as(t)\n",
        "    indices = torch.argmax(t * idx, 1, keepdim=True)\n",
        "    return indices\n",
        "'''\n",
        "def mrr(y_pred, y_true, mask):\n",
        "    # Set the largest value in the ground truth to 1, and everything else to 0.\n",
        "    # If `y_true = [0.65, 0.4, 0.1]`, then `binarized_y_true=[1,0,0]`.\n",
        "    binarized_y_true = y_true.ge(y_true.max(dim=-1, keepdim=True).values).long()  #det her er nok ikke nødvendigt, da det er en one-hot vector\n",
        "    y_pred = y_pred.masked_fill(~mask, -1)\n",
        "\n",
        "    # Sort the predictions to get the predicted ranking.\n",
        "    _, rank = y_pred.sort(descending=True, dim=-1)\n",
        "\n",
        "    # Sort the ground truth labels by their predicted rank.\n",
        "    # If `y_pred=[0.25, 0.55, 0.05]` and `binarized_y_true=[1, 0, 0]`,\n",
        "    # then `rank=[1,0,2]` and `ordered_truth=`[0,1,0]`.\n",
        "    ordered_truth = binarized_y_true.gather(-1, rank)\n",
        "\n",
        "    # Ordered indices: [1, 2, ..., batch_size]\n",
        "    print('y_pred',y_pred.shape)\n",
        "    print('binarized_y_true', binarized_y_true.shape)\n",
        "\n",
        "    ab = torch.arange(1, binarized_y_true.size(-1))\n",
        "    cd = torch.arange(1, binarized_y_true.size(-1)+1)\n",
        "    print('ab', ab.shape)\n",
        "    print('cd', cd.shape)\n",
        "    #indices = torch.arange(1, binarized_y_true.size(-1) + 1).view_as(y_pred).type_as(y_pred)\n",
        "\n",
        "    indices = torch.arange(1, binarized_y_true.size(-1) + 1).view_as(y_pred).type_as(y_pred)\n",
        "\n",
        "    # Calculate the reciprocal rank for each position, 0ing-out masked values.\n",
        "    # Following above example, `_mrr = [0/1, 1/2 ,0/3] = [0, 0.5, 0]`.\n",
        "    #_mrr = (ordered_truth / indices) * mask\n",
        "    _mrr = np.true_divide(ordered_truth, indices) * mask\n",
        "\n",
        "    # Get the mrr at the first non-zero position for each instance in the batch, and take the mean.\n",
        "    # Following above example, `first_nonzero(ordered_truth) = 1`, so we grab `mrr[1] = torch.tensor(0.5)`.\n",
        "    # The example only has one instance, but this works for batched input also.\n",
        "    return _mrr.gather(-1, first_nonzero(ordered_truth)).mean()\n",
        "'''\n",
        "def mrr(y_pred, y_true, mask):\n",
        "    y_pred = y_pred.masked_fill(~mask, -1)\n",
        "    y_true = y_true.ge(y_true.max(dim=-1, keepdim=True).values).float()\n",
        "\n",
        "    _, idx = y_pred.sort(descending=True, dim=-1)\n",
        "    ordered_truth = y_true.gather(1, idx)\n",
        "    \n",
        "    gold = torch.arange(y_true.size(-1)).unsqueeze(0).type_as(y_true)\n",
        "    _mrr = (ordered_truth / (gold + 1)) * mask\n",
        "    \n",
        "    return _mrr.gather(1, first_nonzero(ordered_truth)).mean()\n",
        "\n",
        "class RankingMetric(Metric):\n",
        "    def __init__(\n",
        "        self,\n",
        "        padding_value: int = -1\n",
        "    ) -> None:\n",
        "        self._padding_value = padding_value\n",
        "        self.reset()\n",
        "        \n",
        "    def __call__(\n",
        "            self,\n",
        "            predictions: torch.LongTensor,\n",
        "            gold_labels: torch.LongTensor,\n",
        "            mask: torch.LongTensor = None\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        predictions : ``torch.Tensor``, required.\n",
        "            A tensor of real-valued predictions of shape (batch_size, slate_length).\n",
        "        gold_labels : ``torch.Tensor``, required.\n",
        "            A tensor of real-valued labels of shape (batch_size, slate_length).\n",
        "        \"\"\"\n",
        "        \n",
        "        if mask is None:\n",
        "            mask = torch.ones_like(gold_labels).bool()\n",
        "        \n",
        "        self._all_predictions.append(predictions.detach().cpu())\n",
        "        self._all_gold_labels.append(gold_labels.detach().cpu()) \n",
        "        self._all_masks.append(mask.detach().cpu())\n",
        "        \n",
        "    @property\n",
        "    def predictions(self):\n",
        "        return torch.cat(self._all_predictions, dim=0)\n",
        "    \n",
        "    @property\n",
        "    def gold_labels(self):\n",
        "        return torch.cat(self._all_gold_labels, dim=0)\n",
        "    \n",
        "    @property\n",
        "    def masks(self):\n",
        "        return torch.cat(self._all_masks, dim=0)\n",
        "        \n",
        "    def get_metric(self, reset: bool = False):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def reset(self):\n",
        "        self._all_predictions = []\n",
        "        self._all_gold_labels = []\n",
        "        self._all_masks = []\n",
        "    \n",
        "#@Metric.register(\"mrr\")\n",
        "class MRR(RankingMetric):\n",
        "    def get_metric(self, reset: bool = False):\n",
        "        predictions = torch.cat(self._all_predictions, dim=0)\n",
        "        labels = torch.cat(self._all_gold_labels, dim=0)\n",
        "        masks = torch.cat(self._all_masks, dim=0)\n",
        "\n",
        "        score = mrr(predictions, labels, masks).item()\n",
        "\n",
        "        if reset:\n",
        "            self.reset()\n",
        "        return score"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cesdIodnZF-Q"
      },
      "source": [
        "class RelevanceMatcher(Registrable, torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = torch.nn.Linear(input_dim, 1, bias=False)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        query_embeddings: TextFieldTensors, \n",
        "        candidates_embeddings: TextFieldTensors,\n",
        "        query_mask: torch.Tensor = None,\n",
        "        candidates_mask: torch.Tensor = None\n",
        "    ):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "#@RelevanceMatcher.register('bert_cls')\n",
        "class BertCLS(RelevanceMatcher):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(input_dim=input_dim*4, **kwargs)\n",
        "      \n",
        "        # Gets the [CLS] token from BERT\n",
        "        self._seq2vec_encoder = ClsPooler(embedding_dim=input_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        lyric_embeddings: torch.Tensor,\n",
        "        option_embeddings: torch.Tensor,\n",
        "        lyric_mask: torch.Tensor = None,\n",
        "        option_mask: torch.Tensor = None\n",
        "    )-> torch.Tensor:\n",
        "\n",
        "        '''\n",
        "        lyric_embeddings.data.shape = [batch*num_options, words, dim]\n",
        "        options_embeddings.data.shape = [batch*num_options, words, dim]\n",
        "        lyric_mask.size = [batch*num_options, words]\n",
        "        option_mask.size = [batch*num_options, words]\n",
        "        '''\n",
        "\n",
        "        lyric_encoded = self._seq2vec_encoder(lyric_embeddings, lyric_mask)\n",
        "        option_encoded = self._seq2vec_encoder(option_embeddings, option_mask)\n",
        "\n",
        "        interaction_vector = torch.cat(\n",
        "            [\n",
        "                lyric_encoded,\n",
        "                option_encoded,\n",
        "                torch.abs(lyric_encoded-option_encoded),\n",
        "                lyric_encoded*option_encoded\n",
        "            ],\n",
        "            dim=1,\n",
        "        )\n",
        "        dense_out = self.dense(interaction_vector)\n",
        "        score = torch.squeeze(dense_out,1)\n",
        "\n",
        "        return score\n",
        "\n",
        "\n",
        "#@Model.register(\"ranker\")\n",
        "class DocumentRanker(Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Vocabulary,\n",
        "        text_field_embedder: TextFieldEmbedder,\n",
        "        relevance_matcher: RelevanceMatcher,\n",
        "        dropout: float = None,\n",
        "        num_labels: int = None,\n",
        "        initializer: InitializerApplicator = InitializerApplicator(),\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__(vocab, **kwargs)\n",
        "        self._text_field_embedder = text_field_embedder\n",
        "        self._relevance_matcher = TimeDistributed(relevance_matcher) \n",
        "        self._mrr = MRR(padding_value=-1)\n",
        "        self._loss = torch.nn.MSELoss(reduction='none')\n",
        "        initializer(self)\n",
        "        \n",
        "    # @torchsnooper.snoop()\n",
        "    def forward(  # type: ignore\n",
        "        self, \n",
        "        lyric: TextFieldTensors,               # batch * words\n",
        "        title_options: TextFieldTensors,       # batch * num_options * words\n",
        "        labels: torch.IntTensor = None         # batch * num_options\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "      \n",
        "        embedded_text = self._text_field_embedder(lyric, num_wrapping_dims=0)\n",
        "        mask = get_text_field_mask(lyric).long()\n",
        "\n",
        "        embedded_options = self._text_field_embedder(title_options, num_wrapping_dims=1) \n",
        "        options_mask = get_text_field_mask(title_options, num_wrapping_dims=1).long() #har til føjet det med num_wrapping_dim\n",
        "\n",
        "        #make pairs of (lyric, title) instead of (lyric, (title1, title2,...))\n",
        "        embedded_text = embedded_text.unsqueeze(1).expand(-1, embedded_options.size(1), -1, -1)\n",
        "        mask = mask.unsqueeze(1).expand(-1, embedded_options.size(1), -1)\n",
        "        \n",
        "        '''\n",
        "        embedded_text.data.shape = [batch, num_options, words, dim]\n",
        "        embedded_options.data.shape = [batch, num_options, words, dim]\n",
        "        mask.size = [batch, num_options, words]\n",
        "        options_mask.size = [batch, num_options, words]\n",
        "        '''\n",
        "        scores = self._relevance_matcher(embedded_text, embedded_options, mask, options_mask).squeeze(-1)\n",
        "        probs = torch.sigmoid(scores)\n",
        "\n",
        "        output_dict = {\"logits\": scores, \"probs\": probs}\n",
        "        output_dict[\"token_ids\"] = util.get_token_ids_from_text_field_tensors(lyric)\n",
        "        \n",
        "        if labels is not None:\n",
        "            label_mask = (labels != -1)\n",
        "            self._mrr(probs, labels, label_mask)\n",
        "            probs = probs.view(-1)\n",
        "            labels = labels.view(-1)\n",
        "            label_mask = label_mask.view(-1)   \n",
        "            loss = self._loss(probs, labels)\n",
        "            output_dict[\"loss\"] = loss.masked_fill(~label_mask, 0).sum() / label_mask.sum()\n",
        "\n",
        "        return output_dict\n",
        "\n",
        "    #@overrides\n",
        "    def make_output_human_readable(\n",
        "        self, output_dict: Dict[str, torch.Tensor]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        return output_dict\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        metrics = {\n",
        "            \"mrr\": self._mrr.get_metric(reset)  \n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    #default_predictor = \"document_ranker\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5uNSGKxsD_I",
        "cellView": "code"
      },
      "source": [
        "#@title builds \n",
        "def build_dataset_reader() -> DatasetReader:\n",
        "    return DatasetReaderRanker()\n",
        "\n",
        "def read_data(reader: DatasetReader) -> Tuple[Iterable[Instance], Iterable[Instance]]:\n",
        "    print(\"Reading data\")\n",
        "    training_data = reader.read('/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_train.csv')\n",
        "    validation_data = reader.read('/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_dev.csv') \n",
        "    return training_data , validation_data\n",
        "\n",
        "def build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n",
        "    print(\"Building the vocabulary\")\n",
        "    return Vocabulary.from_instances(instances)\n",
        "\n",
        "def build_model(vocab: Vocabulary) -> Model:\n",
        "    print(\"Building the model\")\n",
        "    vocab_size = vocab.get_vocab_size(\"tokens\")\n",
        "    embedder = BasicTextFieldEmbedder(\n",
        "        {\"tokens\": Embedding(embedding_dim=10, num_embeddings=vocab_size)})\n",
        "    encoder = BagOfEmbeddingsEncoder(embedding_dim=10)\n",
        "    relevanceMatcher = BertCLS(input_dim=10)\n",
        "    return DocumentRanker(vocab, embedder,relevanceMatcher)\n",
        "\n",
        "def build_data_loaders(\n",
        "    train_data: torch.utils.data.Dataset,\n",
        "    dev_data: torch.utils.data.Dataset) -> Tuple[allennlp.data.DataLoader, allennlp.data.DataLoader]:\n",
        "    train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=allennlp_collate)\n",
        "    dev_loader = DataLoader(dev_data, batch_size=8, shuffle=False, collate_fn=allennlp_collate)\n",
        "    return train_loader, dev_loader\n",
        "\n",
        "def build_trainer(\n",
        "    model: Model,\n",
        "    serialization_dir: str,\n",
        "    train_loader: DataLoader,\n",
        "    dev_loader: DataLoader) -> Trainer:\n",
        "    parameters = [\n",
        "        [n, p]\n",
        "        for n, p in model.named_parameters() if p.requires_grad\n",
        "    ]\n",
        "    optimizer = AdamOptimizer(parameters)\n",
        "    trainer = GradientDescentTrainer(\n",
        "        model=model,\n",
        "        serialization_dir=serialization_dir,\n",
        "        data_loader=train_loader,\n",
        "        validation_data_loader=dev_loader,\n",
        "        num_epochs=5,\n",
        "        optimizer=optimizer,\n",
        "    )\n",
        "    return trainer\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czQozK72VBzE",
        "cellView": "code",
        "outputId": "2f3c13bb-4685-40eb-815f-d38ca380c2f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Training loop \n",
        "def run_training_loop():\n",
        "    dataset_reader = build_dataset_reader()\n",
        "\n",
        "    train_data, dev_data = read_data(dataset_reader)\n",
        "\n",
        "    vocab = build_vocab(train_data + dev_data)\n",
        "    model = build_model(vocab)\n",
        "\n",
        "    # This is the allennlp-specific functionality in the Dataset object;\n",
        "    # we need to be able convert strings in the data to integers, and this\n",
        "    # is how we do it.\n",
        "    train_data.index_with(vocab)\n",
        "    dev_data.index_with(vocab)\n",
        "\n",
        "    train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
        "\n",
        "    # You obviously won't want to create a temporary file for your training\n",
        "    # results, but for execution in binder for this guide, we need to do this.\n",
        "    with tempfile.TemporaryDirectory() as serialization_dir:\n",
        "        trainer = build_trainer(\n",
        "            model,\n",
        "            serialization_dir,\n",
        "            train_loader,\n",
        "            dev_loader\n",
        "        )\n",
        "        print(\"Starting training\")\n",
        "        trainer.train()\n",
        "        print(\"Finished training\")\n",
        "\n",
        "    return model, dataset_reader\n",
        "\n",
        "run_training_loop()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "reading instances: 0it [00:00, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Reading data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "reading instances: 113it [19:22, 10.28s/it]\n",
            "  0%|          | 0/26 [10:01<?, ?it/s]\n",
            "\n",
            "reading instances: 53it [00:00, 172.44it/s]\u001b[A\n",
            "reading instances: 126it [00:00, 223.58it/s]\u001b[A\n",
            "reading instances: 206it [00:00, 302.43it/s]\n",
            "reading instances: 70it [00:00, 1140.54it/s]\n",
            "building vocab: 100%|##########| 276/276 [00:00<00:00, 2149.84it/s]\n",
            "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building the vocabulary\n",
            "Building the model\n",
            "Starting training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mrr: 0.0432, batch_loss: 0.2417, loss: 0.2456 ||: 100%|##########| 26/26 [00:02<00:00, 10.74it/s]\n",
            "mrr: 0.1032, batch_loss: 0.2452, loss: 0.2448 ||: 100%|##########| 9/9 [00:00<00:00, 72.81it/s]\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "mrr: 0.0834, batch_loss: 0.2256, loss: 0.2342 ||: 100%|##########| 26/26 [00:01<00:00, 13.15it/s]\n",
            "mrr: 0.1084, batch_loss: 0.2390, loss: 0.2379 ||: 100%|##########| 9/9 [00:00<00:00, 95.91it/s]\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "mrr: 0.0894, batch_loss: 0.2015, loss: 0.2143 ||: 100%|##########| 26/26 [00:01<00:00, 13.43it/s]\n",
            "mrr: 0.1244, batch_loss: 0.2294, loss: 0.2270 ||: 100%|##########| 9/9 [00:00<00:00, 90.24it/s]\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "mrr: 0.0853, batch_loss: 0.1717, loss: 0.1851 ||: 100%|##########| 26/26 [00:01<00:00, 13.45it/s]\n",
            "mrr: 0.1277, batch_loss: 0.2168, loss: 0.2124 ||: 100%|##########| 9/9 [00:00<00:00, 97.23it/s]\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "mrr: 0.0835, batch_loss: 0.1292, loss: 0.1495 ||: 100%|##########| 26/26 [00:01<00:00, 13.09it/s]\n",
            "mrr: 0.1487, batch_loss: 0.2024, loss: 0.1960 ||: 100%|##########| 9/9 [00:00<00:00, 95.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DocumentRanker(\n",
              "   (_text_field_embedder): BasicTextFieldEmbedder(\n",
              "     (token_embedder_tokens): Embedding()\n",
              "   )\n",
              "   (_relevance_matcher): TimeDistributed(\n",
              "     (_module): BertCLS(\n",
              "       (dense): Linear(in_features=40, out_features=1, bias=False)\n",
              "       (_seq2vec_encoder): ClsPooler()\n",
              "     )\n",
              "   )\n",
              "   (_loss): MSELoss()\n",
              " ), <__main__.DatasetReaderRanker at 0x7ff9cc04b208>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VScLR_pr5J8"
      },
      "source": [
        "# Evaluation of the model \n",
        "Computation of evaluation metrics against train set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcar7ktuqd3j",
        "outputId": "172cca1e-3df6-4d45-80fa-a7a8bc649bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model, dataset_reader = run_training_loop()\n",
        "\n",
        "# Now we can evaluate the model on a new dataset.\n",
        "test_data = dataset_reader.read('/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_test.csv')\n",
        "test_data.index_with(model.vocab)\n",
        "data_loader = DataLoader(test_data, batch_size=8, collate_fn=allennlp_collate)\n",
        "\n",
        "results = evaluate(model, data_loader)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rreading instances: 0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Reading data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "reading instances: 206it [00:00, 282.54it/s]\n",
            "reading instances: 70it [00:00, 306.57it/s]\n",
            "building vocab: 100%|##########| 276/276 [00:00<00:00, 2217.13it/s]\n",
            "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building the vocabulary\n",
            "Building the model\n",
            "Starting training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mrr: 0.0299, batch_loss: 0.2429, loss: 0.2481 ||: 100%|##########| 26/26 [00:02<00:00, 12.02it/s]\n",
            "mrr: 0.0844, batch_loss: 0.2472, loss: 0.2469 ||: 100%|##########| 9/9 [00:00<00:00, 71.04it/s]\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "mrr: 0.0413, batch_loss: 0.2327, loss: 0.2385 ||: 100%|##########| 26/26 [00:01<00:00, 13.16it/s]\n",
            "mrr: 0.0886, batch_loss: 0.2413, loss: 0.2407 ||: 100%|##########| 9/9 [00:00<00:00, 95.99it/s]\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "mrr: 0.0527, batch_loss: 0.2113, loss: 0.2216 ||: 100%|##########| 26/26 [00:01<00:00, 13.33it/s]\n",
            "mrr: 0.1074, batch_loss: 0.2318, loss: 0.2303 ||: 100%|##########| 9/9 [00:00<00:00, 96.19it/s]\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "mrr: 0.0661, batch_loss: 0.1783, loss: 0.1948 ||: 100%|##########| 26/26 [00:01<00:00, 13.20it/s]\n",
            "mrr: 0.0988, batch_loss: 0.2182, loss: 0.2155 ||: 100%|##########| 9/9 [00:00<00:00, 93.24it/s]\n",
            "unable to check gpu_memory_mb() due to occasional failure, continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 415, in gpu_memory_mb\n",
            "    encoding=\"utf-8\",\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "mrr: 0.0763, batch_loss: 0.1452, loss: 0.1602 ||: 100%|##########| 26/26 [00:02<00:00, 13.00it/s]\n",
            "mrr: 0.0965, batch_loss: 0.2017, loss: 0.1973 ||: 100%|##########| 9/9 [00:00<00:00, 90.74it/s]\n",
            "reading instances: 0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "reading instances: 70it [00:00, 88.25it/s]\n",
            "mrr: 0.09, loss: 0.20 ||: 100%|##########| 9/9 [00:00<00:00, 74.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxCTtisSYf8p"
      },
      "source": [
        "#Train the model \n",
        "\n",
        "This is for training with commands, when using a configuration file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVvsJLKmYjaJ"
      },
      "source": [
        "#Just tring out different things \n",
        "\n",
        "#path to training configurations: /content/gdrive/My\\ Drive/Bachelor_Project/training_config\n",
        "!allennlp train /content/gdrive/My\\ Drive/Bachelor_Project/training_config/retrieval_config.jsonnet -s /content/gdrive/My\\ Drive/Bachelor_Project/Output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l8weqtYalHC"
      },
      "source": [
        "# Shape: (batch_size, num_tokens, embedding_dim)\n",
        "        #embedded_title = self.embedder(title)  \n",
        "        embedded_lyric = self.embedder(lyric)\n",
        "\n",
        "        # Shape: (batch_size, num_tokens)\n",
        "        #mask_title = util.get_text_field_mask(title)\n",
        "        mask_lyric = util.get_text_field_mask(lyric)\n",
        "\n",
        "\n",
        "        # Shape: (batch_size, encoding_dim)\n",
        "        #encoded_title = self.encoder(embedded_title, mask_title) #We now have a single vector for each batch of instances\n",
        "        encoded_lyric = self.encoder(embedded_lyric, mask_lyric)\n",
        "\n",
        "        # Shape: (batch_size, num_labels)\n",
        "        logits = self.classifier(encoded_lyric)\n",
        "        # Shape: (batch_size, num_labels)\n",
        "        probs = torch.nn.functional.softmax(logits)  #Get a probability distribution over labels \n",
        "        # Shape: (1,)\n",
        "        loss = torch.nn.functional.cross_entropy(logits, title)  #Cross entrophy as loss function \n",
        "        self.accuracy(logits, title)\n",
        "\n",
        "        return {'loss': loss, 'probs': probs}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}