{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRkCH7g+SYJMw5l/cHT72s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatRitchie/Bachelor/blob/main/seq2seq_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwhuHriCGPeP"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCyMQ_ADg7pJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b15e81e-94df-4870-9689-21e6ec02cc1c"
      },
      "source": [
        "!pip install folium==0.2.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting folium==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/dd/75ced7437bfa7cb9a88b96ee0177953062803c3b4cde411a97d98c35adaf/folium-0.2.1.tar.gz (69kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 15.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from folium==0.2.1) (2.11.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2->folium==0.2.1) (1.1.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-cp36-none-any.whl size=79980 sha256=f1bfd00cd2fbaeb69b83307de50b9ad00bf9c739e22f5cfa2fcd7201ad4c6ed0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/09/f0/52d2ef419c2aaf4fb149f92a33e0008bdce7ae816f0dd8f0c5\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvwEWi2efR9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8798cc74-2361-440a-cb82-2d87e4504a9e"
      },
      "source": [
        "!pip install --upgrade urllib3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting urllib3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
            "\r\u001b[K     |██▍                             | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20kB 21.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 40kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 122kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 133kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 6.5MB/s \n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed urllib3-1.26.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yXNPmGkfhl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "284d6580-3019-41ae-be8b-0b93e54aba4d"
      },
      "source": [
        "!pip install registrable"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting registrable\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/47/6370d9389664f1af31de4048934faeb681035a079ddc3d284a366875e755/registrable-0.0.4-py3-none-any.whl\n",
            "Installing collected packages: registrable\n",
            "Successfully installed registrable-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO6nbdH3IBQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899c18bc-95ed-42f0-9c2c-7f49eceb6888"
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/f5/f4dd3424b3ae9dec0a55ae7f7f34ada3ee60e4b10a187d2ba7384c698e09/allennlp-1.3.0-py3-none-any.whl (506kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 6.4MB/s \n",
            "\u001b[?25hCollecting transformers<4.1,>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 28.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<1.8.0,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.7.0+cu101)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 28.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 31.9MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.12)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/0d/fa9ae0872f47fc5804c414a24318651aa101bd973039c832a4b383abee6f/boto3-1.16.45-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8)\n",
            "Requirement already satisfied: spacy<2.4,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.2.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<4.1,>=4.0->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<4.1,>=4.0->allennlp) (20.8)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (0.16.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (1.0.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (20.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (51.0.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (3.3.0)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.45\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/c3/1cbe252d7d3674901ca148d26a005d7d2a37eb867e670e4b3ed8e93796f8/botocore-1.19.45-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<4.1,>=4.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<4.1,>=4.0->allennlp) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.45->boto3<2.0,>=1.14->allennlp) (2.8.1)\n",
            "Building wheels for collected packages: jsonnet, overrides, sacremoses\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp36-cp36m-linux_x86_64.whl size=3387954 sha256=d8eab9ef0dc80add168637b4034127b34c699dbff5275264320dfc3c52249c86\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10175 sha256=d2e0a96fd1b5a590ea88dbe63ee48f3eaefd9ade3d286f13c0ed7bbcc7c7049f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=8d73123185e8f5abf6bd46f25a4fe5695f6c1c3abefab338ab152f4d4929663f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built jsonnet overrides sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, jsonnet, tensorboardX, overrides, sentencepiece, jsonpickle, jmespath, urllib3, botocore, s3transfer, boto3, allennlp\n",
            "  Found existing installation: urllib3 1.26.2\n",
            "    Uninstalling urllib3-1.26.2:\n",
            "      Successfully uninstalled urllib3-1.26.2\n",
            "Successfully installed allennlp-1.3.0 boto3-1.16.45 botocore-1.19.45 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-1.4.2 overrides-3.1.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 tensorboardX-2.1 tokenizers-0.9.4 transformers-4.0.1 urllib3-1.25.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X58EMH44oJ3r",
        "outputId": "22aa06fa-21b5-4651-e685-9b2ad5c63aef"
      },
      "source": [
        "!pip install --pre allennlp-models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp-models\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/e8/3eab9b645a1bd4abac892229952572cd8df5b5de3bd316774f845cbd10f1/allennlp_models-1.3.0-py3-none-any.whl (378kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 5.2MB/s \n",
            "\u001b[?25hCollecting py-rouge==1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.7MB/s \n",
            "\u001b[?25hCollecting conllu==4.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/20/39bf21e3a0304c874c40c9cec96e3f70d2ef4b1ada3585f7dbee91dc8c05/conllu-4.2.1-py2.py3-none-any.whl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: allennlp<1.4,>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from allennlp-models) (1.3.0)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp-models) (3.2.5)\n",
            "Requirement already satisfied: torch<1.8.0,>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from allennlp-models) (1.7.0+cu101)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp-models) (0.2.5)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (1.16.45)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (4.41.1)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (3.1.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (0.8)\n",
            "Requirement already satisfied: spacy<2.4,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (2.2.4)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (1.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (0.1.94)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (2.10.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (2.23.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (3.6.4)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (0.17.0)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (1.19.4)\n",
            "Requirement already satisfied: transformers<4.1,>=4.0 in /usr/local/lib/python3.6/dist-packages (from allennlp<1.4,>=1.3.0->allennlp-models) (4.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->allennlp-models) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.7.0->allennlp-models) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.7.0->allennlp-models) (0.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3<2.0,>=1.14->allennlp<1.4,>=1.3.0->allennlp-models) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3<2.0,>=1.14->allennlp<1.4,>=1.3.0->allennlp-models) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.45 in /usr/local/lib/python3.6/dist-packages (from boto3<2.0,>=1.14->allennlp<1.4,>=1.3.0->allennlp-models) (1.19.45)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (51.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp<1.4,>=1.3.0->allennlp-models) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp<1.4,>=1.3.0->allennlp-models) (3.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp<1.4,>=1.3.0->allennlp-models) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp<1.4,>=1.3.0->allennlp-models) (3.12.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp<1.4,>=1.3.0->allennlp-models) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp<1.4,>=1.3.0->allennlp-models) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp<1.4,>=1.3.0->allennlp-models) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp<1.4,>=1.3.0->allennlp-models) (2.10)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp<1.4,>=1.3.0->allennlp-models) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp<1.4,>=1.3.0->allennlp-models) (8.6.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp<1.4,>=1.3.0->allennlp-models) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp<1.4,>=1.3.0->allennlp-models) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp<1.4,>=1.3.0->allennlp-models) (1.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<4.1,>=4.0->allennlp<1.4,>=1.3.0->allennlp-models) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<4.1,>=4.0->allennlp<1.4,>=1.3.0->allennlp-models) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers<4.1,>=4.0->allennlp<1.4,>=1.3.0->allennlp-models) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<4.1,>=4.0->allennlp<1.4,>=1.3.0->allennlp-models) (0.0.43)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.45->boto3<2.0,>=1.14->allennlp<1.4,>=1.3.0->allennlp-models) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp<1.4,>=1.3.0->allennlp-models) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<4.1,>=4.0->allennlp<1.4,>=1.3.0->allennlp-models) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<4.1,>=4.0->allennlp<1.4,>=1.3.0->allennlp-models) (7.1.2)\n",
            "Building wheels for collected packages: ftfy, word2number\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45613 sha256=095a810befafadb66cb79220599a96210a57bea4729aeb46eaae3fc94c99344c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=a1331cfcb190381aafbe13df0f3d09e51a3a81bd1ac4ab4abb3df64c38e99ffa\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "Successfully built ftfy word2number\n",
            "Installing collected packages: py-rouge, conllu, ftfy, word2number, allennlp-models\n",
            "Successfully installed allennlp-models-1.3.0 conllu-4.2.1 ftfy-5.8 py-rouge-1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4yQED8vGZuo"
      },
      "source": [
        "# Mounting google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VzxyUCgLG3Q",
        "outputId": "bfb85f30-b057-40b9-f947-f26b198daa15"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvjtf1JeGy8G"
      },
      "source": [
        "# Train model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT83bZa16QB6",
        "outputId": "c2ed2999-df53-4e3d-a0f7-0669d6b3e14c"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Bachelor_Project/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Bachelor_Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d20Ev23BajnF"
      },
      "source": [
        "!allennlp train '/content/gdrive/My Drive/Bachelor_Project/training_config/seq2seq_config.json' -s Output_seq2seq/ -f "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9IcCbKQG3SW"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zLW4iv1anay"
      },
      "source": [
        "!allennlp evaluate 'Output_seq2seq/model.tar.gz' '/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_test.csv' --include-package information_retrieval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOoAAAvixBv9"
      },
      "source": [
        "!allennlp evaluate 'Output_seq2seq/model.tar.gz' '/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_test_without_title.csv' --include-package information_retrieval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZElYOyCkG6bi"
      },
      "source": [
        "# Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQbINZ_0aunF"
      },
      "source": [
        "!allennlp predict  'Output_seq2seq/model.tar.gz' 'predictions/rejoice_seq.json'  --predictor seq2seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z146p3MsmPcS"
      },
      "source": [
        "import itertools\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "import allennlp\r\n",
        "from allennlp_models.generation.dataset_readers import Seq2SeqDatasetReader\r\n",
        "#from allennlp.data.iterators import BucketIterator\r\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\r\n",
        "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\r\n",
        "from allennlp.data.vocabulary import Vocabulary\r\n",
        "from allennlp.nn.activations import Activation\r\n",
        "from allennlp_models.generation.models.simple_seq2seq import SimpleSeq2Seq\r\n",
        "from allennlp.modules.attention import LinearAttention, BilinearAttention, DotProductAttention\r\n",
        "from allennlp_models.rc.modules import StackedSelfAttentionEncoder #PytorchSeq2SeqWrapper, \r\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\r\n",
        "from allennlp.modules.token_embedders import Embedding\r\n",
        "from allennlp_models.generation.predictors import Seq2SeqPredictor\r\n",
        "from allennlp.training.trainer import Trainer\r\n",
        "from allennlp.training.trainer import Trainer, GradientDescentTrainer\r\n",
        "\r\n",
        "from allennlp.data.tokenizers import (\r\n",
        "    Token, \r\n",
        "    Tokenizer,\r\n",
        "    CharacterTokenizer,\r\n",
        "    PretrainedTransformerTokenizer,\r\n",
        "    SpacyTokenizer,\r\n",
        "    WhitespaceTokenizer,\r\n",
        ")\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from allennlp.data import allennlp_collate\r\n",
        "from allennlp.models import Model\r\n",
        "import tempfile\r\n",
        "from allennlp.training.optimizers import AdamOptimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTU233bHv9VK",
        "outputId": "046b67a9-39c3-4e01-b659-63a81e0190f5"
      },
      "source": [
        "reader = Seq2SeqDatasetReader(\r\n",
        "          source_tokenizer=WhitespaceTokenizer(),\r\n",
        "          delimiter=\",\",\r\n",
        "          source_token_indexers={'tokens': SingleIdTokenIndexer()},\r\n",
        "          target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\r\n",
        "train_dataset = reader.read('/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_train.csv')\r\n",
        "validation_dataset = reader.read('/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_train.csv')\r\n",
        "\r\n",
        "print(\"dataset\", train_dataset[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading instances: 217it [00:00, 1922.56it/s]\n",
            "reading instances: 217it [00:00, 2181.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset Instance with fields:\n",
            " \t source_tokens: TextField of length 207 with text: \n",
            " \t\t[@start@, across, the, sea, by, the, tennessee, skyline, they, told, me, id, find, my, hopes, and,\n",
            "\t\tmy, dreams, but, i, long, to, be, in, the, bed, of, my, true, love, back, where, i, came, from,\n",
            "\t\tshes, waiting, for, me, so, ill, make, my, way, through, long, winding, country, roads, but, my,\n",
            "\t\theart, still, beats, for, my, home, and, my, english, rose, i, told, my, dad, on, the, phone, its,\n",
            "\t\tamazing, from, the, straight, to, the, craziest, places, ive, seen, but, i, long, to, be, in, the,\n",
            "\t\tarms, of, my, true, love, like, he, loves, my, mother, he, understands, me, i, spend, my, days,\n",
            "\t\tjust, traveling, and, playing, shows, but, my, heart, still, beats, for, my, home, and, my, english,\n",
            "\t\trose, i, met, a, man, in, the, bar, down, in, memphis, he, told, me, he, went, there, to, follow,\n",
            "\t\this, dreams, he, told, me, son, you, know, i, lost, my, true, love, for, the, same, exact, reason,\n",
            "\t\tthat, you, crossed, the, sea, and, i, found, truth, in, people, i, barely, know, but, my, heart,\n",
            "\t\tstill, beats, for, my, home, and, my, english, rose, oh, my, heart, still, beats, for, my, home,\n",
            "\t\tand, my, english, rose, oh, my, heart, still, beats, for, my, home, and, my, english, rose, @end@]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
            " \t target_tokens: TextField of length 4 with text: \n",
            " \t\t[@start@, english, rose, @end@]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exJL8kwumJ1z",
        "outputId": "44a8a8cf-bdd0-4824-d6fc-aa9028a6013a"
      },
      "source": [
        "EN_EMBEDDING_DIM = 256\r\n",
        "ZH_EMBEDDING_DIM = 256\r\n",
        "HIDDEN_DIM = 256\r\n",
        "CUDA_DEVICE = 0\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def build_data_loaders(train_data: torch.utils.data.Dataset, dev_data: torch.utils.data.Dataset):\r\n",
        "  train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=allennlp_collate)\r\n",
        "  dev_loader = DataLoader(dev_data, batch_size=8, shuffle=False, collate_fn=allennlp_collate)\r\n",
        "  return train_loader, dev_loader\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def build_trainer(model: Model, serialization_dir: str, train_loader: DataLoader, dev_loader: DataLoader) -> Trainer: \r\n",
        "      parameters = [\r\n",
        "                    [n, p] for n, p in model.named_parameters() if p.requires_grad\r\n",
        "      ]\r\n",
        "      optimizer = AdamOptimizer(parameters)\r\n",
        "      trainer = GradientDescentTrainer(\r\n",
        "            model=model,\r\n",
        "            serialization_dir=serialization_dir,\r\n",
        "            data_loader=train_loader,\r\n",
        "            validation_data_loader=dev_loader,\r\n",
        "            num_epochs=1,\r\n",
        "            optimizer=optimizer\r\n",
        "          )\r\n",
        "      return trainer\r\n",
        "\r\n",
        "def run_training_loop():\r\n",
        "    \r\n",
        "      reader = Seq2SeqDatasetReader(\r\n",
        "          source_tokenizer=WhitespaceTokenizer(),\r\n",
        "          delimiter=\",\",\r\n",
        "          source_token_indexers={'tokens': SingleIdTokenIndexer()},\r\n",
        "          target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\r\n",
        "      train_dataset = reader.read('/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_train.csv')\r\n",
        "      validation_dataset = reader.read('/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_train.csv')\r\n",
        "\r\n",
        "      print(\"dataset\", train_dataset)\r\n",
        "\r\n",
        "      vocab = Vocabulary.from_instances(train_dataset + validation_dataset,\r\n",
        "                                        min_count={'tokens': 3, 'target_tokens': 3})\r\n",
        "        \r\n",
        "        \r\n",
        "      train_dataset.index_with(vocab)\r\n",
        "      validation_dataset.index_with(vocab)\r\n",
        "\r\n",
        "      train_loader, dev_loader = build_data_loaders(train_dataset, validation_dataset)\r\n",
        "\r\n",
        "      en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\r\n",
        "                             embedding_dim=EN_EMBEDDING_DIM)\r\n",
        "   \r\n",
        "      encoder = StackedSelfAttentionEncoder(input_dim=EN_EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, projection_dim=128, feedforward_hidden_dim=128, num_layers=1, num_attention_heads=8)\r\n",
        "      source_embedder = BasicTextFieldEmbedder({\"tokens\": en_embedding})\r\n",
        "      attention = DotProductAttention()\r\n",
        "      max_decoding_steps = 20 \r\n",
        "\r\n",
        "      model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\r\n",
        "                          target_embedding_dim=ZH_EMBEDDING_DIM,\r\n",
        "                          target_namespace='target_tokens',\r\n",
        "                          attention=attention,\r\n",
        "                          beam_size=8,\r\n",
        "                          use_bleu=True)\r\n",
        "       \r\n",
        "      with tempfile.TemporaryDirectory() as serialization_dir:\r\n",
        "            trainer = build_trainer(\r\n",
        "                model,\r\n",
        "                serialization_dir,\r\n",
        "                train_loader,\r\n",
        "                dev_loader\r\n",
        "            )\r\n",
        "            print(\"Starting training\")\r\n",
        "            trainer.train()\r\n",
        "            print(\"Finished training\")\r\n",
        "            print('Epoch: {}'.format(i))\r\n",
        "\r\n",
        "            \r\n",
        "    \r\n",
        "      return model, reader\r\n",
        "\r\n",
        "model, reader = run_training_loop()\r\n",
        "\r\n",
        "validation_dataset = reader.read('/content/gdrive/My Drive/Kaggle/LYRICS_DATASET_train.csv')\r\n",
        "predictor = Seq2SeqPredictor(model, reader)\r\n",
        "\r\n",
        "for instance in itertools.islice(validation_dataset, 10):\r\n",
        "    print('SOURCE:', instance.fields['source_tokens'].tokens)\r\n",
        "    print('GOLD:', instance.fields['target_tokens'].tokens)\r\n",
        "    print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading instances: 217it [00:00, 1687.67it/s]\n",
            "reading instances: 217it [00:00, 557.74it/s]\n",
            "building vocab: 100%|##########| 434/434 [00:00<00:00, 4726.85it/s]\n",
            "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "  0%|          | 0/28 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset <allennlp.data.dataset_readers.dataset_reader.AllennlpDataset object at 0x7f75104442e8>\n",
            "Starting training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "batch_loss: 1.8928, loss: 2.6547 ||: 100%|##########| 28/28 [00:30<00:00,  1.10s/it]\n",
            "BLEU: 0.1431, batch_loss: 3.5739, loss: 2.3224 ||: 100%|##########| 28/28 [00:12<00:00,  2.21it/s]\n",
            "reading instances: 217it [00:00, 2186.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished training\n",
            "Epoch: 0\n",
            "SOURCE: [@start@, Lyric, @end@]\n",
            "GOLD: [@start@, Title, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n",
            "SOURCE: [@start@, you, dont, have, to, say, a, thing, you, dont, have, to, share, the, pain, id, bring, and, i, dont, need, to, see, through, walls, when, the, metadata, says, it, all, postin, pictures, smiles, and, beaches, tennis, shoes, and, matching, tshirts, its, out, of, my, hands, but, its, in, my, phone, when, the, metadata, says, it, all, faith, in, timing, some, go, your, way, and, others, stay, in, place, ill, be, fighting, pain, and, the, patience, pain, and, the, patience, what, more, can, i, do, now, but, hold, on, strong, take, it, day, by, day, its, all, your, call, i, know, silence, when, its, standing, tall, and, the, metadata, says, it, all, faith, in, timing, some, go, your, way, and, others, stay, in, place, ill, be, fighting, pain, and, the, patience, pain, and, the, patience, pain, and, the, patience, pain, and, the, patience, @end@]\n",
            "GOLD: [@start@, metadata, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n",
            "SOURCE: [@start@, i, took, the, supermarket, flowers, from, the, windowsill, i, threw, the, day, old, tea, from, the, cup, packed, up, the, photo, album, matthew, had, made, memories, of, a, life, thats, been, loved, took, the, get, well, soon, cards, and, stuffed, animals, poured, the, old, ginger, beer, down, the, sink, dad, always, told, me, dont, you, cry, when, youre, down, but, mum, theres, a, tear, every, time, that, i, blink, oh, im, in, pieces, its, tearing, me, up, but, i, know, a, heart, thats, broke, is, a, heart, thats, been, loved, so, ill, sing, hallelujah, you, were, an, angel, in, the, shape, of, my, mum, when, i, fell, down, youd, be, there, holding, me, up, spread, your, wings, as, you, go, when, god, takes, you, back, hell, say, hallelujah, youre, home, i, fluffed, the, pillows, made, the, beds, stacked, the, chairs, up, folded, your, nightgowns, neatly, in, a, case, john, says, hed, drive, then, put, his, hand, on, my, cheek, and, wiped, a, tear, from, the, side, of, my, face, i, hope, that, i, see, the, world, as, you, did, cause, i, know, a, life, with, love, is, a, life, thats, been, lived, so, ill, sing, hallelujah, you, were, an, angel, in, the, shape, of, my, mum, when, i, fell, down, youd, be, there, holding, me, up, spread, your, wings, as, you, go, when, god, takes, you, back, hell, say, hallelujah, youre, home, hallelujah, you, were, an, angel, in, the, shape, of, my, mum, you, got, to, see, the, person, i, have, become, spread, your, wings, and, i, know, that, when, god, took, you, back, he, said, hallelujah, youre, home, @end@]\n",
            "GOLD: [@start@, supermarket, flowers, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n",
            "SOURCE: [@start@, across, the, sea, by, the, tennessee, skyline, they, told, me, id, find, my, hopes, and, my, dreams, but, i, long, to, be, in, the, bed, of, my, true, love, back, where, i, came, from, shes, waiting, for, me, so, ill, make, my, way, through, long, winding, country, roads, but, my, heart, still, beats, for, my, home, and, my, english, rose, i, told, my, dad, on, the, phone, its, amazing, from, the, straight, to, the, craziest, places, ive, seen, but, i, long, to, be, in, the, arms, of, my, true, love, like, he, loves, my, mother, he, understands, me, i, spend, my, days, just, traveling, and, playing, shows, but, my, heart, still, beats, for, my, home, and, my, english, rose, i, met, a, man, in, the, bar, down, in, memphis, he, told, me, he, went, there, to, follow, his, dreams, he, told, me, son, you, know, i, lost, my, true, love, for, the, same, exact, reason, that, you, crossed, the, sea, and, i, found, truth, in, people, i, barely, know, but, my, heart, still, beats, for, my, home, and, my, english, rose, oh, my, heart, still, beats, for, my, home, and, my, english, rose, oh, my, heart, still, beats, for, my, home, and, my, english, rose, @end@]\n",
            "GOLD: [@start@, english, rose, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n",
            "SOURCE: [@start@, star, sign, gemini, brown, eyes, fair, hair, in, the, light, we, called, time, last, night, and, i, cant, stop, thinkin, bout, her, and, her, lips, upon, mine, so, soft, feelings, i, dont, know, the, name, of, under, the, clothes, we, take, off, used, to, be, two, hearts, in, love, oh, why, oh, why, am, i, alone, did, i, did, i, do, something, wrong, am, i, the, reason, or, have, you, found, someone, else, so, tell, me, girl, how, can, i, live, without, love, how, can, i, be, what, you, want, cause, when, the, morning, comes, around, youre, still, gone, and, ill, say, how, can, i, see, through, the, dark, all, i, can, do, is, wonder, where, you, are, are, you, happy, in, someone, elses, arms, well, thats, the, way, to, break, my, heart, thats, the, way, to, break, my, heart, thats, the, way, to, break, my, heart, thats, the, way, to, break, my, heart, thats, the, way, to, break, my—, first, love, never, dies, guess, ill, see, you, in, another, life, 12, years, down, the, line, theres, just, one, thing, i, remember, her, lips, upon, mine, so, soft, feelings, i, dont, know, the, name, of, under, the, clothes, we, take, off, used, to, be, two, hearts, in, love, oh, why, oh, why, am, i, alone, did, i, did, i, do, something, wrong, am, i, the, reason, or, have, you, found, someone, else, so, tell, me, girl, how, can, i, live, without, love, how, can, i, be, what, you, want, cause, when, the, morning, comes, around, youre, still, gone, and, ill, say, how, can, i, see, through, the, dark, all, i, can, do, is, wonder, where, you, are, are, you, happy, in, someone, elses, arms, well, thats, the, way, to, break, my, heart, thats, the, way, to, break, my, heart, thats, the, way, to, break, my, heart, thats, the, way, to, break, my, heart, thats, the, way, to, break, my—, thats, the—, thats, the—, thats, the—, did, i, did, i, thats, the—, thats, the—, thats, the, way, to, break, my—, @end@]\n",
            "GOLD: [@start@, way, to, break, my, heart, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n",
            "SOURCE: [@start@, only, a, nascent, trying, to, harness, huge, fire, out, on, the, beach, in, the, darkness, starting, bonfire, so, gorgeous, a, man, might, cry, burning, trees, in, the, basement, start, a, cool, fire, feel, my, heartbeat, racing, baby, youre, on, fire, so, gorgeous, a, man, might, cry, back, in, paris, you, told, me, you, were, suicidal, its, not, a, vacation, if, i, lose, you, to, the, eiffel, youre, gorgeous, but, you, cant, fly, a, hidden, admirer, sent, me, roses, white, as, fire, we, took, our, handfuls, it, was, war, flower, fighter, wildfire, wildfire, wildfire, wildfire, @end@]\n",
            "GOLD: [@start@, wildfire, interlude, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n",
            "SOURCE: [@start@, the, club, isnt, the, best, place, to, find, a, lover, so, the, bar, is, where, i, go, me, and, my, friends, at, the, table, doing, shots, drinking, fast, and, then, we, talk, slow, and, you, come, over, and, start, up, a, conversation, with, just, me, and, trust, me, ill, give, it, a, chance, now, take, my, hand, stop, put, van, the, man, on, the, jukebox, and, then, we, start, to, dance, and, now, im, singing, like, girl, you, know, i, want, your, love, your, love, was, handmade, for, somebody, like, me, come, on, now, follow, my, lead, i, may, be, crazy, dont, mind, me, say, boy, lets, not, talk, too, much, grab, on, my, waist, and, put, that, body, on, me, come, on, now, follow, my, lead, come, come, on, now, follow, my, lead, im, in, love, with, the, shape, of, you, we, push, and, pull, like, a, magnet, do, although, my, heart, is, falling, too, im, in, love, with, your, body, and, last, night, you, were, in, my, room, and, now, my, bed, sheets, smell, like, you, every, day, discovering, something, brand, new, im, in, love, with, your, body, oh—i—oh—i—oh—i—oh—i, im, in, love, with, your, body, oh—i—oh—i—oh—i—oh—i, im, in, love, with, your, body, oh—i—oh—i—oh—i—oh—i, im, in, love, with, your, body, every, day, discovering, something, brand, new, im, in, love, with, the, shape, of, you, one, week, in, we, let, the, story, begin, were, going, out, on, our, first, date, you, and, me, are, thrifty, so, go, all, you, can, eat, fill, up, your, bag, and, i, fill, up, a, plate, we, talk, for, hours, and, hours, about, the, sweet, and, the, sour, and, how, your, family, is, doing, okay, leave, and, get, in, a, taxi, then, kiss, in, the, backseat, tell, the, driver, make, the, radio, play, and, im, singing, like, girl, you, know, i, want, your, love, your, love, was, handmade, for, somebody, like, me, come, on, now, follow, my, lead, i, may, be, crazy, dont, mind, me, say, boy, lets, not, talk, too, much, grab, on, my, waist, and, put, that, body, on, me, come, on, now, follow, my, lead, come, come, on, now, follow, my, lead, im, in, love, with, the, shape, of, you, we, push, and, pull, like, a, magnet, do, although, my, heart, is, falling, too, im, in, love, with, your, body, and, last, night, you, were, in, my, room, and, now, my, bed, sheets, smell, like, you, every, day, discovering, something, brand, new, im, in, love, with, your, body, oh—i—oh—i—oh—i—oh—i, im, in, love, with, your, body, oh—i—oh—i—oh—i—oh—i, im, in, love, with, your, body, oh—i—oh—i—oh—i—oh—i, im, in, love, with, your, body, every, day, discovering, something, brand, new, im, in, love, with, the, shape, of, you, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, im, in, love, with, the, shape, of, you, we, push, and, pull, like, a, magnet, do, although, my, heart, is, falling, too, im, in, love, with, your, body, last, night, you, were, in, my, room, and, now, my, bed, sheets, smell, like, you, every, day, discovering, something, brand, new, im, in, love, with, your, body, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, im, in, love, with, your, body, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, im, in, love, with, your, body, come, on, be, my, baby, come, on, come, on, be, my, baby, come, on, im, in, love, with, your, body, every, day, discovering, something, brand, new, im, in, love, with, the, shape, of, you, @end@]\n",
            "GOLD: [@start@, shape, of, you, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n",
            "SOURCE: [@start@, its, a, friday, we, finally, made, it, i, cant, believe, i, get, to, see, your, face, youve, been, working, and, ive, been, waiting, to, pick, you, up, and, take, you, from, this, place, love, on, the, weekend, love, on, the, weekend, like, only, we, can, like, only, we, can, love, on, the, weekend, love, on, the, weekend, im, coming, up, and, im, loving, every, minute, of, it, you, be, the, dj, ill, be, the, driver, you, put, your, feet, up, in, the, getaway, car, im, flying, fast, like, a, a, wanted, man, i, want, you, baby, like, you, cant, understand, oh, love, on, the, weekend, love, on, the, weekend, we, found, a, message, in, a, bottle, we, were, drinking, love, on, the, weekend, love, on, the, weekend, i, hate, your, guts, cause, im, loving, every, minute, of, it, oh, oh, oh, oh, oh, oh, oh, i, gotta, leave, ya, its, gonna, hurt, me, my, clothes, are, dirty, and, my, friends, are, getting, worried, down, there, below, us, under, the, clouds, baby, take, my, hand, and, pull, me, down, down, down, down, and, ill, be, dreamin, of, the, next, time, we, can, go, into, another, serotonin, overflow, love, on, the, weekend, love, on, the, weekend, im, busted, up, but, im, loving, every, minute, of, it, love, on, the, weekend, love, on, the, weekend, im, looking, for, a, little, love, im, looking, for, a, little, love, oh, yeah, love, on, the, weekend, love, on, the, weekend, love, on, the, weekend, @end@]\n",
            "GOLD: [@start@, love, on, the, weekend, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n",
            "SOURCE: [@start@, on, my, way, and, i, had, to, lie, hows, it, going;, im, doing, fine, and, im, on, my, way, home, im, on, my, way, home, heres, a, friend, that, you, cant, ignore, fight, breaks, out, then, im, out, the, door, and, im, on, my, way, home, im, on, my, way, home, oh, no, its, in, your, eyes, youll, never, be, the, same, but, im, on, my, way, home, yes, im, on, my, way, home, yes, im, on, my, way, home, up, for, show, then, the, stage, went, black, lit, the, joint, then, i, hit, the, sack, and, im, on, my, way, home, im, on, my, way, home, caught, the, flight, and, i, lost, my, name, transatlantic, i, hope, today, that, im, on, my, way, home, im, on, my, way, home, oh, no, its, in, your, eyes, youll, never, be, the, same, but, im, on, my, way, home, yes, im, on, my, way, home, yes, im, on, my, way, home, here, we, stand, in, the, customs, line, he, says, youre, packing;, ill, pull, you, back, and, im, on, my, way, home, im, on, my, way, home, detained, now, and, im, feeling, low, hoping, some, day, theyll, let, me, go, and, im, on, my, way, home, im, on, my, way, home, oh, no, its, in, your, eyes, youll, never, be, the, same, but, im, on, my, way, home, yes, im, on, my, way, home, yes, im, on, my, way, home, @end@]\n",
            "GOLD: [@start@, on, my, way, home, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n",
            "SOURCE: [@start@, tryin, hard, to, recognise, some, pure, motive, inside, of, me, a, creature, that, would, horrify, any, child, that, i, used, to, be, oh, give, no, faith, to, show, started, to, smile, so, i, showed, my, teeth, no, more, than, flesh, and, bone, doin, so, much, just, to, watch, someone, bleed, but, i, love, the, very, blood, of, you, it, keeps, its, heat, in, spite, of, you, oh, the, heart, that, beats, to, keep, you, here, with, me, always, tryin, hard, i, nearly, find, innocence, in, spite, of, me, oh, creature, that, would, terrify, any, child, left, inside, of, me, oh, thats, a, cold, insight, nothin, above, us, and, nothin, below, ahh, but, you, might, be, right, if, there, is, no, heaven, and, there, is, no, soul, i, just, love, the, very, blood, of, you, it, keeps, its, heat, in, spite, of, you, oh, the, heart, that, beats, to, keep, you, here, with, me, always, always, always, always, oh, i, love, the, very, blood, of, you, it, keeps, its, heat, in, spite, of, you, oh, the, heart, that, beats, to, keep, you, here, with, me, always, oh, i, love, the, very, blood, of, you, leaks, and, squeeks, you, black, and, blue, oh, the, heart, that, beats, to, keep, you, here, with, me, always, @end@]\n",
            "GOLD: [@start@, blood, @end@]\n",
            "PRED: [['@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@'], [], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@'], ['@@UNKNOWN@@', 'the'], ['@@UNKNOWN@@', 'you'], ['@@UNKNOWN@@', 'the', '@@UNKNOWN@@'], ['@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@', '@@UNKNOWN@@']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}